From bf1170f8eeb3b413f6695de7947765fbc7c80148 Mon Sep 17 00:00:00 2001
From: qchenao <oxymoron@sjtu.edu.cn>
Date: Sat, 3 Sep 2016 13:27:35 -0700
Subject: [PATCH 17/17] add python data layer for Render4CNN

---
 python/HardExamMinLayer.py | 163 +++++++++++++++++++++++++++++++++++++++++++++
 python/MySoftmaxLayer.py   |  67 +++++++++++++++++++
 python/lmdb_reader.py      |  52 +++++++++++++++
 python/share_data.py       |  15 +++++
 4 files changed, 297 insertions(+)
 create mode 100644 python/HardExamMinLayer.py
 create mode 100644 python/MySoftmaxLayer.py
 create mode 100644 python/lmdb_reader.py
 create mode 100644 python/share_data.py

diff --git a/python/HardExamMinLayer.py b/python/HardExamMinLayer.py
new file mode 100644
index 0000000..1998344
--- /dev/null
+++ b/python/HardExamMinLayer.py
@@ -0,0 +1,163 @@
+import caffe
+import numpy as np
+import random
+import os, struct
+from array import array
+from share_data import imgnet_mean
+from lmdb_reader import Read_Render4CNN
+import pdb
+class Render4CNNLayer(caffe.Layer):
+
+    def setup(self, bottom, top):
+
+        self.idx = []
+        self.data = []
+        params = eval(self.param_str_)
+        self.source = params['source']
+        self.init = params.get('init', True)
+        self.seed = params.get('seed', None)
+        self.batch_size=params.get('batch_size', 64)
+
+        # two tops: data and label
+        if len(top) != 1:
+            raise Exception("Need to define 1 top: data or label.")
+        # data layers have no bottoms
+        if len(bottom) != 0:
+            raise Exception("Do not define a bottom.")
+
+
+        self.idx = np.array(range(self.batch_size))
+
+
+
+    def reshape(self, bottom, top):
+
+        self.data = Read_Render4CNN(self.source,self.idx)
+
+        if 'image' in self.source:
+            self.data = self.data.reshape(self.batch_size,3,227,227)
+            self.data -= imgnet_mean
+            top[0].reshape(self.batch_size,3,227,227)
+        else:
+            top[0].reshape(self.batch_size,4,1,1)
+            self.data = self.data.reshape(self.batch_size,4,1,1)
+
+
+    def forward(self, bottom, top):
+        # assign output
+        top[0].data[...] = self.data
+
+
+        self.idx = (self.idx + self.batch_size) % 2314401
+
+    def backward(self, top, propagate_down, bottom):
+        pass
+
+
+
+
+class MNISTLayer_hard(caffe.Layer):
+
+    def setup(self, bottom, top):
+
+        self.idx = []
+        self.data = []
+        self.label = []
+
+        params = eval(self.param_str)
+        self.mnist_dir = params['mnist_dir']
+        self.split = params['split']
+        #self.idx = np.array(params['idx'])
+        self.init = params.get('init', True)
+        self.seed = params.get('seed', None)
+        self.batch_size=params.get('batch_size', 64)
+
+        # two tops: data and label
+        if len(top) != 2:
+            raise Exception("Need to define two tops: data and label.")
+        # data layers have no bottoms
+        if len(bottom) != 0:
+            raise Exception("Do not define a bottom.")
+
+        # make eval deterministic
+        #if 'training' not in self.split:
+            #self.init = False
+
+        # randomization: seed and pick
+        if self.split is 'training':
+            #random.seed(self.seed)
+            #self.idx = np.random.randint(0,59999,size=self.batch_size)
+            self.idx = np.array(range(self.batch_size))
+        else:
+            #random.seed(self.seed)
+            #self.idx = np.random.randint(0,9999,size=self.batch_size)
+            self.idx = np.array(range(self.batch_size))
+
+
+
+    def reshape(self, bottom, top):
+
+        if self.split is 'training':
+            self.data = share_data.ims_training[self.idx]
+            self.label = share_data.labels_training[self.idx]
+        else:
+            self.data = share_data.ims_testing[self.idx]
+            self.label= share_data.labels_testing[self.idx]
+
+
+        # reshape tops to fit (leading 1 is for batch dimension)
+        top[0].reshape(self.data.shape[0],1,self.data.shape[1],self.data.shape[2])
+        top[1].reshape(self.label.shape[0])
+
+        self.data = self.data.reshape(self.data.shape[0], 1, self.data.shape[1], self.data.shape[2])
+        #im = np.array(self.data[0][0],dtype='uint8')
+        #img = Image.fromarray(im)
+        #img.show()
+        self.label = self.label.reshape(self.label.shape[0])
+
+
+    def forward(self, bottom, top):
+        # assign output
+        top[0].data[...] = self.data
+        top[1].data[...] = self.label
+        # top[0].data = self.data
+        # top[1].data = self.label
+
+        # pick next input
+        #self.idx=share_data.vars
+        if self.split is 'training':
+            if share_data.count < 1000:
+                self.idx = (self.idx + self.batch_size) % 60000
+            else:
+                if share_data.flag :
+                    if (share_data.count // 1000) % 2:
+                        self.idx = np.arange(self.batch_size)
+                    else:
+                        share_data.idx_pool = np.argsort(share_data.data_loss,axis=-1,kind='quicksort',order=None)
+                        share_data.idx_pool = share_data.idx_pool[np.arange(30000,60000)]
+                        self.idx = share_data.idx_pool[np.arange(self.batch_size)]
+                        #np.save('data_loss',share_data.data_loss)
+                        #np.save('idx_pool',share_data.idx_pool)
+                else:
+                    if  ((share_data.count // 1000) % 2):
+                        self.idx = (self.idx + self.batch_size) % 60000
+                    else:
+		                self.idx = share_data.idx_pool[np.arange(self.batch_size)+ (share_data.count % 500)*self.batch_size]
+
+
+
+
+
+            #self.idx = (self.idx + self.batch_size) % 60000
+            #print 'training_index'
+            #print self.idx
+        else:
+            random.seed(self.seed)
+            #self.idx = np.random.randint(0,9999,size=self.batch_size)
+            #print 'testing_index'
+            #print self.idx
+
+            self.idx = (self.idx + self.batch_size) % 10000
+
+    def backward(self, top, propagate_down, bottom):
+        pass
diff --git a/python/MySoftmaxLayer.py b/python/MySoftmaxLayer.py
new file mode 100644
index 0000000..7fb53d9
--- /dev/null
+++ b/python/MySoftmaxLayer.py
@@ -0,0 +1,67 @@
+import numpy as np
+import caffe
+
+
+
+class MySoftmaxLayer(caffe.Layer):
+
+    def setup(self, bottom, top):
+
+        # check input pair
+        if len(bottom) != 2:
+            raise Exception("Need two inputs to compute distance.")
+	    #params = eval(self.param_str)
+        #self.split = params['split']
+
+    def reshape(self, bottom, top):
+
+        # check input dimensions match
+        if bottom[0].num != bottom[1].num:
+            raise Exception("Inputs must have the same dimension.")
+            #raise Exception("Inputs must have the same dimension.")
+        # difference is shape of inputs
+        self.diff = np.zeros_like(bottom[0].data, dtype=np.float32)
+        # loss output is scalar
+        top[0].reshape(1)
+
+    def forward(self, bottom, top):
+	#print 'num',bottom[0].num
+        scores = np.array(bottom[0].data)
+        #print 'bottom',bottom[0].data.shape,bottom[0].data
+        tmp = np.tile(np.max(scores,axis=1),np.max(bottom[1].data).astype(int)+1)
+        tmp = tmp.reshape(scores.T.shape).T
+        #print 'tmp',tmp.shape,tmp
+        #print 'scores',scores.shape,scores
+        scores = scores-tmp
+        exp_scores = np.exp(scores)
+        #64*10
+        #tmp = np.sum(exp_scores, axis=1, keepdims=True)
+        #tmp = np.tile(tmp,10).reshape(scores.T.shape).T
+        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
+        correct_logprobs = -np.log(probs[range(bottom[0].num),np.array(bottom[1].data,dtype=np.uint16).reshape(bottom[1].num)]+10**(-10))
+        #print 'correct',probs[range(bottom[0].num),np.array(bottom[1].data,dtype=np.uint16).reshape(bottom[1].num)]+10**(-10)
+        #print 'log',correct_logprobs
+        data_loss = np.sum(correct_logprobs)/bottom[0].num
+        '''
+        if self.split is 'training':
+            if share_data.flag:
+                share_data.data_loss = []
+            share_data.data_loss = np.append(share_data.data_loss, correct_logprobs)
+	    #print 'loss', share_data.data_loss.shape
+        '''
+
+        self.diff[...] = probs
+        top[0].data[...] = data_loss
+
+
+    def backward(self, top, propagate_down, bottom):
+
+        delta = self.diff
+
+
+        #for i in range(2):
+        if propagate_down[1]:
+            raise Exception("Layer cannot backprop to label inputs.")
+        if propagate_down[0]:
+            delta[range(bottom[0].num), np.array(bottom[1].data,dtype=np.uint16).reshape(bottom[1].num)] -= 1
+            bottom[0].diff[...] = delta/bottom[0].num
diff --git a/python/lmdb_reader.py b/python/lmdb_reader.py
new file mode 100644
index 0000000..0181504
--- /dev/null
+++ b/python/lmdb_reader.py
@@ -0,0 +1,52 @@
+import caffe
+import lmdb
+import numpy as np
+import matplotlib.pyplot as plt
+from caffe.proto import caffe_pb2
+# Wei Yang 2015-08-19
+# Source
+#   Read LevelDB/LMDB
+#   ==================
+#       http://research.beenfrog.com/code/2015/03/28/read-leveldb-lmdb-for-caffe-with-python.html
+#   Plot image
+#   ==================
+#       http://www.pyimagesearch.com/2014/11/03/display-matplotlib-rgb-image/
+#   Creating LMDB in python
+#   ==================
+#       http://deepdish.io/2015/04/28/creating-lmdb-in-python/
+
+
+def Read_Render4CNN(lmdb_file, index):
+
+    lmdb_env = lmdb.open(lmdb_file)
+    lmdb_txn = lmdb_env.begin(buffers=True)
+    datum = caffe_pb2.Datum()
+    data = []
+    for ind in index:
+
+        buf = lmdb_txn.get('%010d'%ind)
+        datum.ParseFromString(bytes(buf))
+        if (ind!=datum.label):
+            raise Exception('index wrong')
+        tmp = caffe.io.datum_to_array(datum).astype(np.uint8)
+        data = np.append(data,tmp)
+        #im = data.astype(np.uint8)
+        #im = np.transpose(im, (2, 1, 0)) # original (dim, col, row)
+        #print "index ", ind
+        #print "data", data
+
+    return data
+    #lmdb_cursor = lmdb_txn.cursor()
+
+
+    #for key, value in lmdb_cursor:
+    #     datum.ParseFromString(value)
+    #
+    #     index = datum.label
+    #     data = caffe.io.datum_to_array(datum)
+    #     #im = data.astype(np.uint8)
+    #     #im = np.transpose(im, (2, 1, 0)) # original (dim, col, row)
+    #     print "index ", index
+    #     print "data", data
+    #     ##plt.imshow(im)
+    #     #plt.show()
diff --git a/python/share_data.py b/python/share_data.py
new file mode 100644
index 0000000..10c1ef7
--- /dev/null
+++ b/python/share_data.py
@@ -0,0 +1,15 @@
+#from mnist import *
+import numpy as np
+# MNIST = mnist()
+# ims_training,labels_training = MNIST.load_training()
+# ims_testing,labels_testing = MNIST.load_testing()
+
+data_loss=np.array([])
+count=0
+flag=0
+idx_pool=np.array([])
+
+imgnet_mean=np.ndarray(shape=(3,227,227),dtype=int)
+imgnet_mean[0] = 104
+imgnet_mean[1] = 117
+imgnet_mean[2] = 123
-- 
1.9.1

